{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#importing dependencies import numpy as np import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import Normalizer from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler from sklearn import svm\nfrom google.colab import drive drive.mount('/content/drive')\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n#loading dataset to a pandas dataframe credit_card_data=pd.read_csv('/content/drive/MyDrive/credit_card_datas et/creditcard.csv')\n#print dimension of dataset\nprint(credit_card_data.shape) (284807, 31)\n#first 5 rows of data set\ncredit_card_data.head()\n#last 5\trows\ncredit_card_data.tail()\n#get some information about dataset\ncredit_card_data.info()\n#there is also another method to find the number of missing values\ncredit_card_data.isnull().sum()\n#checking the distribution of legit transaction and fradulant transaction\ncredit_card_data['Class'].value_counts()\n#seperate normal and fradulant transaction from the data frame. #create 2 variables legit and fraud\nlegit = credit_card_data[credit_card_data.Class == 0] #if class value is 0, that entire row will be stored in legit variable\nfraud = credit_card_data[credit_card_data.Class == 1]\nprint(legit.shape) print(fraud.shape)\n(284315, 31)\n(492, 31)\n#to get statistical measures of legit data\nlegit.Amount.describe()\n#to get statistical measures of fraud data\nfraud.Amount.describe()\n#compare the values of both transation\ncredit_card_data.groupby('Class').mean()\n# in the 284315 legit transactions, randomly pick 492 transactions and join with 492 fradulant transactions ,thus the data set can be balanced.\n\nlegit_sample=legit.sample(n=492)\nnew_dataset = pd.concat([legit_sample , fraud], axis=0)\t#concatenate\n492 legit and 492 fradulant dataframe into new dataset #if axis=0, data frame to be added 1 by 1\n#if axis=1, the values will be added column wise\n#check first 5 rows of new dataset\nnew_dataset.head()\n#here the serial numbers , we can see they are randomly..\n#check last 5 rows\nnew_dataset.tail()\nnew_dataset['Class'].value_counts() \nnew_dataset.groupby('Class').mean()\n# here the different in mean is same as that of last time ,thus we find that nature of dataset is not changed.\n#since the mean values are similar ,our sample is good.\n#splitting data into features and target {to feed to machinelearning model}\n\nX = new_dataset.drop(columns = 'Class', axis = 1) Y = new_dataset['Class']\nprint(X)\n#here the class column is removed\nprint(Y)\n# Split data into training and testing data\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2 , stratify=Y, random_state=2)\nprint(X.shape, X_train.shape, X_test.shape) \nscaler = StandardScaler() scaler.fit(X) StandardScaler()\nstandardized_data = scaler.transform(X) print(standardized_data)\nX = standardized_data\nY = new_dataset['Class']\nprint(X)\t#data\nprint(Y)\t#label\nclassifier = svm.SVC(kernel='linear')\n\nclassifier.fit(X_train, Y_train) SVC(kernel='linear')\n#Logistic Regression\n\n#create variable as model\nmodel = LogisticRegression()\n#training the logistic regression model with training data\n\nmodel.fit(X_train, Y_train)\n#x_train contain all features of training data, y_train contain corresponding labels (0 /1)\n#this will fit data in logistic regression model and then we can make some predictionsfrom it.\nLogisticRegression()\n\n#accuracy score on training data\nX_train_prediction = model.predict(X_train)\ntraining_data_accuracy = accuracy_score(X_train_prediction, Y_train) print('Accuracy on Training Data : ', training_data_accuracy)\n#accuracy score on training data\nX_test_prediction = model.predict(X_test)\ntest_data_accuracy = accuracy_score(X_test_prediction, Y_test) print('Accuracy on Test Data : ', test_data_accuracy)\n\n#HISTOGRAM\n#credit_card dataset\ndef draw_histograms(dataframe, features, rows, cols):\t#call draw histogram funtction\nfig=plt.figure(figsize=(20,20))\t#give figure\n \nsize\n \n\nfor i, feature in enumerate(features): ax=fig.add_subplot(rows,cols,i+1)\n \n\ndataframe[feature].hist(bins=25,ax=ax,facecolor='midnightblue')\n\nax.set_title(feature+\" Distribution\",color='DarkRed') ax.set_yscale('log')\nfig.tight_layout() plt.show()\ndraw_histograms(credit_card_data,credit_card_data.columns,8,4)\n#HISTOGRAM\n#new_dataset\ndef draw_histograms(dataframe, features, rows, cols): fig=plt.figure(figsize=(20,20))\nfor i, feature in enumerate(features): ax=fig.add_subplot(rows,cols,i+1)\n\ndataframe[feature].hist(bins=25,ax=ax,facecolor='midnightblue')\n\nax.set_title(feature+\" Distribution\",color='DarkRed') ax.set_yscale('log')\n \nfig.tight_layout() plt.show()\n\n\ndraw_histograms(new_dataset,new_dataset.columns,8,4)\n#SCATTER PLOT\n#credit_card_dataset\nfrom matplotlib import rcParams rcParams['figure.figsize']= 8,4\nplt.scatter(credit_card_data.Class, credit_card_data.Amount)\n\n#SCATTER PLOT\n#new_dataset\nrcParams['figure.figsize']= 8,4 plt.scatter(new_dataset.Class, new_dataset.Amount)\n#BOX PLOT\n#Credit_card_data\nplt.boxplot(credit_card_data.Class)\n{'boxes': [<matplotlib.lines.Line2D at 0x7f79586c4050>], 'caps': [<matplotlib.lines.Line2D at 0x7f79586f3210>,\n<matplotlib.lines.Line2D at 0x7f79586f3750>],\n'fliers': [<matplotlib.lines.Line2D at 0x7f79586f9250>], 'means': [],\n'medians': [<matplotlib.lines.Line2D at 0x7f79586f3cd0>],\n \n'whiskers': [<matplotlib.lines.Line2D at 0x7f79586ea750>,\n<matplotlib.lines.Line2D at 0x7f79586eac90>]}\n#BOX PLOT\n#new_dataset\nplt.boxplot(new_dataset.Class)\n{'boxes': [<matplotlib.lines.Line2D at 0x7f79586e27d0>], 'caps': [<matplotlib.lines.Line2D at 0x7f795865c250>,\n<matplotlib.lines.Line2D at 0x7f795865c790>],\n'fliers': [<matplotlib.lines.Line2D at 0x7f7958663290>], 'means': [],\n'medians': [<matplotlib.lines.Line2D at 0x7f795865cd10>], 'whiskers': [<matplotlib.lines.Line2D at 0x7f7958655790>,\n<matplotlib.lines.Line2D at 0x7f7958655cd0>]}\ninput_data = (7,-0.89428608220282,0.286157196276544,-\n0.113192212729871,-\n0.271526130088604,2.6695986595986,3.72181806112751,0.370145127676916,0\n.851084443200905,-0.392047586798604,-0.410430432848439,-\n0.705116586646536,-0.110452261733098,-\n0.286253632470583,0.0743553603016731,-0.328783050303565,-\n0.210077268148783,-\n0.499767968800267,0.118764861004217,0.57032816746536,0.052735669114969\n7,-0.0734251001059225,-0.268091632235551,-\n0.204232669947878,1.0115918018785,0.373204680146282,-\n0.384157307702294,0.0117473564581996,0.14240432992147,93.2)\ninput_data_as_numpy_array = np.asarray(input_data) input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\nstd_data = scaler.transform(input_data_reshaped) print(std_data)\n\nprediction = classifier.predict(std_data) print(prediction)\n\nif (prediction[0] == 0):\nprint('The transaction is not fradulant')\nelse:\nprint('The transaction is fradulant')\n#Model Training\n\n\n#create variable as model\nmodel = LogisticRegression()\n#training the logistic regression model with training data\n\nmodel.fit(X_train, Y_train)\n#x_train contain all features of training data, y_train contain corresponding labels (0 /1)\n#this will fit data in logistic regression model and then we can make some predictionsfrom it.\nLogisticRegression()\ny_train_pred = model.predict(X_train) y_test_pred = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nprint(confusion_matrix(Y_train, y_train_pred)) print(classification_report(Y_train, y_train_pred))\nprint(confusion_matrix(Y_test, y_test_pred)) print(classification_report(Y_test, y_test_pred))\n#Building Decision Tree\tModel\t\t\nfrom sklearn.tree import DecisionTreeClassifier\ndt_clf = DecisionTreeClassifier(criterion = 'gini', max_depth = 20, random_state=0)\ndt_clf.fit(X_train, Y_train) DecisionTreeClassifier(max_depth=20, random_state=0)\nDecision Tree Model Evaluation\nprint(\"Train Results\")\npred_train = dt_clf.predict(X_train)\n\nprint(confusion_matrix(Y_train, pred_train)) print(classification_report(Y_train, pred_train))\nprint(\"Test Results\")\npred_test = dt_clf.predict(X_test)\n\nprint(confusion_matrix(Y_test, pred_test)) print(classification_report(Y_test, pred_test))\nfrom sklearn.model_selection import GridSearchCV from sklearn.ensemble import RandomForestClassifier rf_clf = RandomForestClassifier(random_state=345)\nparam_grid = { 'n_estimators': [50],\n'max_depth' : [8,16,20]\n}\nrf_clf = RandomForestClassifier(n_estimators = 50,max_depth = 20,\nrandom_state=345, verbose = 1)\nrf_clf.fit(X_train, Y_train)\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done\t50 out of\t50 | elapsed:\t0.2s finished\nRandomForestClassifier(max_depth=20, n_estimators=50, random_state=345,verbose=1)\nprint(\"Train Results\")\npred_train = rf_clf.predict(X_train)\n\nprint(confusion_matrix(Y_train, pred_train)) print(classification_report(Y_train, pred_train))\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done\t50 out of\t50 | elapsed:\t0.0s finished\nprint(\"Test Results\")\npred_test = rf_clf.predict(X_test)\n\nprint(confusion_matrix(Y_test, pred_test)) print(classification_report(Y_test, pred_test))\nfrom sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import cross_val_score\n \nerror_rate = []\nfor i in range(1,40):\n\nknn = KNeighborsClassifier(n_neighbors=i) knn.fit(X_train,Y_train)\npred_i = knn.predict(X_test) error_rate.append(np.mean(pred_i != Y_test))\nknn.fit(X_train,Y_train) KNeighborsClassifier(n_neighbors=39) plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed',\nmarker='o',\nmarkerfacecolor='red', markersize=10) plt.title('Error Rate vs. K Value') plt.xlabel('K')\nplt.ylabel('Error Rate') Text(0, 0.5, 'Error Rate')\n\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_blobs from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler from sklearn.metrics import\nconfusion_matrix,classification_report,accuracy_score,roc_curve,roc_au c_score,f1_score, precision_score,\trecall_score, cohen_kappa_score\nkmeans=KMeans(n_clusters=2,random_state=0,algorithm=\"elkan\",max_iter=1 0000)\nkmeans.fit(X_train) kmeans_predicted_train_labels=kmeans.predict(X_train)\nprint(\"tn --> true negatives\") print(\"fp --> false positives\") print(\"fn --> false negatives\") print(\"tp --> true positives\")\ntn,fp,fn,tp=confusion_matrix(Y_train,kmeans_predicted_train_labels).ra vel()\nreassignflag=False\nif tn+tp<fn+fp:\n# clustering is opposite of original classification\nreassignflag=True kmeans_predicted_test_labels=kmeans.predict(X_test) if reassignflag:\n \nkmeans_predicted_test_labels=1-kmeans_predicted_test_labels #calculating confusion matrix for kmeans tn,fp,fn,tp=confusion_matrix(Y_test,kmeans_predicted_test_labels).rave l()\n#scoring kmeans kmeans_accuracy_score=accuracy_score(Y_test,kmeans_predicted_test_labe ls) kmeans_precison_score=precision_score(Y_test,kmeans_predicted_test_lab els) kmeans_recall_score=recall_score(Y_test,kmeans_predicted_test_labels) kmeans_f1_score=f1_score(Y_test,kmeans_predicted_test_labels) kmeans_kappa_score=cohen_kappa_score(Y_test,kmeans_predicted_test_labe ls)\nfpr, tpr, _=roc_curve(Y_test,kmeans_predicted_test_labels) kmeans_auc=roc_auc_score(Y_test,kmeans_predicted_test_labels) #printing\nprint(\"\")\nprint(\"K-Means\") print(\"Confusion Matrix\") print(\"tn =\",tn,\"fp =\",fp)\nprint(\"fn =\",fn,\"tp =\",tp) print(\"Scores\")\nprint(\"Accuracy -->\",kmeans_accuracy_score) print(\"Precison -->\",kmeans_precison_score) print(\"Recall -->\",kmeans_recall_score) print(\"F1 -->\",kmeans_f1_score) print(\"Kappa -->\",kmeans_kappa_score) print('Area Under Curve:',kmeans_auc) print(\"fpr = \", fpr , \"tpr = \", tpr)\n\n\nfrom sklearn.mixture import GaussianMixture\ngm= GaussianMixture(n_components=2, n_init=10, covariance_type='spherical')\ngm.fit(X_train)\nGaussianMixture(covariance_type='spherical', n_components=2, n_init=10)\ngaussian_predicted_train_labels= gm.predict(X_train) print(\"tn --> true negatives\")\nprint(\"fp --> false positives\")\nprint(\"fn --> false negatives\") print(\"tp --> true positives\")\ntn,fp,fn,tp=confusion_matrix(Y_train,gaussian_predicted_train_labels). ravel()\nreassignflag=False\nif tn+tp<fn+fp:\n# clustering is opposite of original classification\nreassignflag=True gaussian_predicted_test_labels=gm.predict(X_test) if reassignflag:\ngaussian_predicted_test_labels=1-gaussian_predicted_test_labels #calculating confusion matrix for kmeans tn,fp,fn,tp=confusion_matrix(Y_test,gaussian_predicted_test_labels).ra vel()\n#scoring kmeans gaussian_accuracy_score=accuracy_score(Y_test,gaussian_predicted_test_ labels) gaussian_precison_score=precision_score(Y_test,gaussian_predicted_test\n_labels) gaussian_recall_score=recall_score(Y_test,gaussian_predicted_test_labe ls)\ngaussian_f1_score=f1_score(Y_test,gaussian_predicted_test_labels) gaussian_kappa_score=cohen_kappa_score(Y_test,gaussian_predicted_test_ labels)\nfpr, tpr, _=roc_curve(Y_test,gaussian_predicted_test_labels) gaussian_auc=roc_auc_score(Y_test,gaussian_predicted_test_labels) #printing\nprint(\"\") print(\"gaussian\") print(\"Confusion Matrix\") print(\"tn =\",tn,\"fp =\",fp)\nprint(\"fn =\",fn,\"tp =\",tp) print(\"Scores\")\nprint(\"Accuracy -->\",gaussian_accuracy_score) print(\"Precison -->\",gaussian_precison_score) print(\"Recall -->\",gaussian_recall_score) print(\"F1 -->\",gaussian_f1_score) print(\"Kappa -->\",gaussian_kappa_score)\n \nprint('Area Under Curve:',gaussian_auc) print(\"fpr = \", fpr , \"tpr = \", tpr)\n\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}